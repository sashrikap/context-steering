{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Steering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we demonstrate using multiple contexts to steer the generation of a single prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cos.utils import load_hf_model_and_tokenizer\n",
    "from cos.core import multi_contextual_steering_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7a81dcf7acc4ba2845856172072aa6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load in the model. You can also experiment with other chat models, such a T0pp and Mistral.\n",
    "model, tokenizer = load_hf_model_and_tokenizer(model_name='llama-2-7b-chat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have the LLM explain Newton's second law to us under two contexts: one where we're a toddler and one where we're a professor. We can also tune the level of influence (i.e. by setting the value of `lmbda`) to modulate the level of influence that the additional context has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts_a = [\n",
    "    \"I want to spend time with families during evenings.\",\n",
    "]\n",
    "contexts_b = [\n",
    "    \"I want to stay late at work to finish the critical projects.\",\n",
    "]\n",
    "prompts = [\n",
    "    \"I have an important deadline tomorrow. What should I do?\",\n",
    "]\n",
    "all_lmbds = [\n",
    "    [0, 1, 2, 0, 1, 2, 0, 1, 2],\n",
    "    [0, 0, 0, 1, 1, 1, 2, 2, 2],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outs = multi_contextual_steering_hf(\n",
    "    model, \n",
    "    tokenizer,\n",
    "    prompts=prompts,\n",
    "    all_contexts=[contexts_a, contexts_b],\n",
    "    all_lambdas=all_lmbds, \n",
    "    put_context_first=True, \n",
    "    max_gen_len=256,\n",
    "    show_progress=False,\n",
    "    verbose=True,\n",
    "    temperature=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(outs[\"generation\"])):\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"lambda a {all_lmbds[0][i]} lambda b {all_lmbds[1][i]}: \\n{outs['generation'][i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
